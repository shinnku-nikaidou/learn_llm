{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af51f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e417763d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(12, dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eaada19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777d64f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D tensor:\n",
      "Shape: torch.Size([12]), numel(): 12\n",
      "\n",
      "2D tensor:\n",
      "Shape: torch.Size([3, 4]), numel(): 12\n",
      "\n",
      "3D tensor:\n",
      "Shape: torch.Size([2, 3, 4]), numel(): 24\n",
      "\n",
      "4D tensor:\n",
      "Shape: torch.Size([1, 2, 3, 4]), numel(): 24\n",
      "\n",
      "For shape (2, 3, 4): 2 × 3 × 4 = 24\n",
      "c.numel() = 24\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate numel() with different shapes\n",
    "print(\"1D tensor:\")\n",
    "a = torch.arange(12)\n",
    "print(f\"Shape: {a.shape}, numel(): {a.numel()}\")\n",
    "\n",
    "print(\"\\n2D tensor:\")\n",
    "b = torch.arange(12).reshape(3, 4)\n",
    "print(f\"Shape: {b.shape}, numel(): {b.numel()}\")\n",
    "\n",
    "print(\"\\n3D tensor:\")\n",
    "c = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"Shape: {c.shape}, numel(): {c.numel()}\")\n",
    "\n",
    "print(\"\\n4D tensor:\")\n",
    "d = torch.arange(24).reshape(1, 2, 3, 4)\n",
    "print(f\"Shape: {d.shape}, numel(): {d.numel()}\")\n",
    "\n",
    "# Show that numel() = product of shape dimensions\n",
    "print(f\"\\nFor shape (2, 3, 4): 2 × 3 × 4 = {2*3*4}\")\n",
    "print(f\"c.numel() = {c.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a93b221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0.,   1.,   2.,   3.,   4.],\n",
      "          [  5.,   6.,   7.,   8.,   9.],\n",
      "          [ 10.,  11.,  12.,  13.,  14.],\n",
      "          [ 15.,  16.,  17.,  18.,  19.]],\n",
      "\n",
      "         [[ 20.,  21.,  22.,  23.,  24.],\n",
      "          [ 25.,  26.,  27.,  28.,  29.],\n",
      "          [ 30.,  31.,  32.,  33.,  34.],\n",
      "          [ 35.,  36.,  37.,  38.,  39.]],\n",
      "\n",
      "         [[ 40.,  41.,  42.,  43.,  44.],\n",
      "          [ 45.,  46.,  47.,  48.,  49.],\n",
      "          [ 50.,  51.,  52.,  53.,  54.],\n",
      "          [ 55.,  56.,  57.,  58.,  59.]]],\n",
      "\n",
      "\n",
      "        [[[ 60.,  61.,  62.,  63.,  64.],\n",
      "          [ 65.,  66.,  67.,  68.,  69.],\n",
      "          [ 70.,  71.,  72.,  73.,  74.],\n",
      "          [ 75.,  76.,  77.,  78.,  79.]],\n",
      "\n",
      "         [[ 80.,  81.,  82.,  83.,  84.],\n",
      "          [ 85.,  86.,  87.,  88.,  89.],\n",
      "          [ 90.,  91.,  92.,  93.,  94.],\n",
      "          [ 95.,  96.,  97.,  98.,  99.]],\n",
      "\n",
      "         [[100., 101., 102., 103., 104.],\n",
      "          [105., 106., 107., 108., 109.],\n",
      "          [110., 111., 112., 113., 114.],\n",
      "          [115., 116., 117., 118., 119.]]]])\n",
      "torch.Size([2, 3, 4, 5])\n",
      "tensor([115., 116., 117., 118., 119.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(120, dtype=torch.float32).reshape(2, 3, 4, 5)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a[1][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf85807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cat() concatenates tensors along a specified dimension\n",
      "\n",
      "Tensor x:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "Tensor y:\n",
      "tensor([[ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "torch.cat([x, y], dim=0) - concatenate along rows:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "Shape: torch.Size([4, 3])\n",
      "\n",
      "torch.cat([x, y], dim=1) - concatenate along columns:\n",
      "tensor([[ 1,  2,  3,  7,  8,  9],\n",
      "        [ 4,  5,  6, 10, 11, 12]])\n",
      "Shape: torch.Size([2, 6])\n",
      "\n",
      "torch.cat([x, y, z], dim=0) - concatenate 3 tensors:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15],\n",
      "        [16, 17, 18]])\n",
      "Shape: torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate torch.cat() - concatenate tensors\n",
    "print(\"torch.cat() concatenates tensors along a specified dimension\")\n",
    "\n",
    "# Create some example tensors\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "y = torch.tensor([[7, 8, 9], \n",
    "                  [10, 11, 12]])\n",
    "\n",
    "print(f\"\\nTensor x:\\n{x}\")\n",
    "print(f\"Shape: {x.shape}\")\n",
    "\n",
    "print(f\"\\nTensor y:\\n{y}\")\n",
    "print(f\"Shape: {y.shape}\")\n",
    "\n",
    "# Concatenate along dimension 0 (rows)\n",
    "cat_dim0 = torch.cat([x, y], dim=0)\n",
    "print(f\"\\ntorch.cat([x, y], dim=0) - concatenate along rows:\\n{cat_dim0}\")\n",
    "print(f\"Shape: {cat_dim0.shape}\")\n",
    "\n",
    "# Concatenate along dimension 1 (columns)  \n",
    "cat_dim1 = torch.cat([x, y], dim=1)\n",
    "print(f\"\\ntorch.cat([x, y], dim=1) - concatenate along columns:\\n{cat_dim1}\")\n",
    "print(f\"Shape: {cat_dim1.shape}\")\n",
    "\n",
    "# Can concatenate multiple tensors\n",
    "z = torch.tensor([[13, 14, 15],\n",
    "                  [16, 17, 18]])\n",
    "cat_multiple = torch.cat([x, y, z], dim=0)\n",
    "print(f\"\\ntorch.cat([x, y, z], dim=0) - concatenate 3 tensors:\\n{cat_multiple}\")\n",
    "print(f\"Shape: {cat_multiple.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995555a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication in PyTorch\n",
      "\n",
      "Matrix A:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Shape: torch.Size([2, 3])\n",
      "\n",
      "Matrix B:\n",
      "tensor([[ 7.,  8.],\n",
      "        [ 9., 10.],\n",
      "        [11., 12.]])\n",
      "Shape: torch.Size([3, 2])\n",
      "\n",
      "==================================================\n",
      "Method 1 - A @ B (@ operator):\n",
      "tensor([[ 58.,  64.],\n",
      "        [139., 154.]])\n",
      "\n",
      "Method 2 - torch.matmul(A, B):\n",
      "tensor([[ 58.,  64.],\n",
      "        [139., 154.]])\n",
      "\n",
      "Method 3 - torch.mm(A, B):\n",
      "tensor([[ 58.,  64.],\n",
      "        [139., 154.]])\n",
      "\n",
      "Method 4 - A.mm(B):\n",
      "tensor([[ 58.,  64.],\n",
      "        [139., 154.]])\n",
      "\n",
      "==================================================\n",
      "All methods give the same result!\n",
      "\n",
      "Element-wise multiplication A * B would fail because shapes don't match:\n",
      "A shape: torch.Size([2, 3]), B shape: torch.Size([3, 2])\n",
      "Element-wise multiplication requires same shapes or broadcasting compatibility\n",
      "\n",
      "==================================================\n",
      "Batch matrix multiplication:\n",
      "batch_A shape: torch.Size([3, 2, 4])\n",
      "batch_B shape: torch.Size([3, 4, 5])\n",
      "batch_A @ batch_B shape: torch.Size([3, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication in PyTorch\n",
    "print(\"Matrix Multiplication in PyTorch\")\n",
    "\n",
    "# Create example matrices with different dimensions\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]], dtype=torch.float32)  # 2x3 matrix\n",
    "B = torch.tensor([[7, 8], \n",
    "                  [9, 10],\n",
    "                  [11, 12]], dtype=torch.float32)    # 3x2 matrix\n",
    "\n",
    "print(f\"\\nMatrix A:\\n{A}\")\n",
    "print(f\"Shape: {A.shape}\")\n",
    "\n",
    "print(f\"\\nMatrix B:\\n{B}\")\n",
    "print(f\"Shape: {B.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Method 1: @ operator (recommended for matrix multiplication)\n",
    "result1 = A @ B\n",
    "print(f\"Method 1 - A @ B (@ operator):\\n{result1}\")\n",
    "\n",
    "# Method 2: torch.matmul() function\n",
    "result2 = torch.matmul(A, B)\n",
    "print(f\"\\nMethod 2 - torch.matmul(A, B):\\n{result2}\")\n",
    "\n",
    "# Method 3: torch.mm() function (only for 2D matrices)\n",
    "result3 = torch.mm(A, B)\n",
    "print(f\"\\nMethod 3 - torch.mm(A, B):\\n{result3}\")\n",
    "\n",
    "# Method 4: A.mm(B) method\n",
    "result4 = A.mm(B)\n",
    "print(f\"\\nMethod 4 - A.mm(B):\\n{result4}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All methods give the same result!\")\n",
    "\n",
    "# Element-wise multiplication (different from matrix multiplication)\n",
    "# Note: A and B have different shapes, so element-wise multiplication won't work\n",
    "print(f\"\\nElement-wise multiplication A * B would fail because shapes don't match:\")\n",
    "print(f\"A shape: {A.shape}, B shape: {B.shape}\")\n",
    "print(\"Element-wise multiplication requires same shapes or broadcasting compatibility\")\n",
    "\n",
    "# Batch matrix multiplication example\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Batch matrix multiplication:\")\n",
    "batch_A = torch.randn(3, 2, 4)  # 3 matrices of size 2x4\n",
    "batch_B = torch.randn(3, 4, 5)  # 3 matrices of size 4x5\n",
    "batch_result = batch_A @ batch_B  # Result: 3 matrices of size 2x5\n",
    "print(f\"batch_A shape: {batch_A.shape}\")\n",
    "print(f\"batch_B shape: {batch_B.shape}\")\n",
    "print(f\"batch_A @ batch_B shape: {batch_result.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6f54bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d883600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aca9911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acb33a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe76ca22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a086a829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_() # pyright: ignore[reportOptionalMemberAccess]\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dee2032b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.detach() explanation:\n",
      "============================================================\n",
      "Original tensor a: tensor([2., 3.], requires_grad=True)\n",
      "a.requires_grad: True\n",
      "\n",
      "After b = a * a:\n",
      "b: tensor([4., 9.], grad_fn=<MulBackward0>)\n",
      "b.requires_grad: True\n",
      "b.grad_fn: <MulBackward0 object at 0x113953d30>\n",
      "\n",
      "After b_detached = b.detach():\n",
      "b_detached: tensor([4., 9.])\n",
      "b_detached.requires_grad: False\n",
      "b_detached.grad_fn: None\n",
      "\n",
      "Key points about detach():\n",
      "1. Returns a new tensor that shares the same data\n",
      "2. But is detached from the computational graph\n",
      "3. requires_grad=False for the detached tensor\n",
      "4. No gradients will flow through the detached tensor\n",
      "\n",
      "============================================================\n",
      "Gradient flow demonstration:\n",
      "With normal tensor - a.grad: tensor([ 8., 12.])\n",
      "Trying to call backward on detached tensor...\n",
      "ERROR: element 0 of tensors does not require grad and does not have a grad_fn\n",
      "This is expected! Detached tensors can't backpropagate gradients\n",
      "\n",
      "Common use cases for detach():\n",
      "- Stopping gradients at certain points\n",
      "- Creating targets for loss functions\n",
      "- Implementing techniques like target networks\n",
      "- Memory optimization in certain scenarios\n"
     ]
    }
   ],
   "source": [
    "# Understanding tensor.detach() - Breaking the computational graph\n",
    "print(\"tensor.detach() explanation:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a tensor that requires gradients\n",
    "a = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "print(f\"Original tensor a: {a}\")\n",
    "print(f\"a.requires_grad: {a.requires_grad}\")\n",
    "\n",
    "# Perform some operations\n",
    "b = a * a\n",
    "print(f\"\\nAfter b = a * a:\")\n",
    "print(f\"b: {b}\")\n",
    "print(f\"b.requires_grad: {b.requires_grad}\")\n",
    "print(f\"b.grad_fn: {b.grad_fn}\")  # Shows the computational graph\n",
    "\n",
    "# Detach b from the computational graph\n",
    "b_detached = b.detach()\n",
    "print(f\"\\nAfter b_detached = b.detach():\")\n",
    "print(f\"b_detached: {b_detached}\")\n",
    "print(f\"b_detached.requires_grad: {b_detached.requires_grad}\")\n",
    "print(f\"b_detached.grad_fn: {b_detached.grad_fn}\")\n",
    "\n",
    "print(f\"\\nKey points about detach():\")\n",
    "print(\"1. Returns a new tensor that shares the same data\")\n",
    "print(\"2. But is detached from the computational graph\")\n",
    "print(\"3. requires_grad=False for the detached tensor\")\n",
    "print(\"4. No gradients will flow through the detached tensor\")\n",
    "\n",
    "# Demonstrate gradient flow\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Gradient flow demonstration:\")\n",
    "\n",
    "# Reset gradients\n",
    "a.grad = None\n",
    "\n",
    "# Case 1: Without detach - gradients flow through\n",
    "c = b * 2  # b still connected to computational graph\n",
    "loss1 = c.sum()\n",
    "loss1.backward()\n",
    "print(f\"With normal tensor - a.grad: {a.grad}\")\n",
    "\n",
    "# Reset gradients\n",
    "a.grad = None\n",
    "\n",
    "# Case 2: With detach - no gradients flow through\n",
    "d = b_detached * 2  # b_detached is disconnected from graph\n",
    "loss2 = d.sum()\n",
    "print(f\"Trying to call backward on detached tensor...\")\n",
    "try:\n",
    "    loss2.backward()\n",
    "    print(f\"With detached tensor - a.grad: {a.grad}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(\"This is expected! Detached tensors can't backpropagate gradients\")\n",
    "\n",
    "print(f\"\\nCommon use cases for detach():\")\n",
    "print(\"- Stopping gradients at certain points\")\n",
    "print(\"- Creating targets for loss functions\")\n",
    "print(\"- Implementing techniques like target networks\")\n",
    "print(\"- Memory optimization in certain scenarios\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
